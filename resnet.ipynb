{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"resnet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jXdp5rLt5Llh","colab_type":"code","outputId":"27ddb4ab-375e-4f88-9eb2-7f35302b4635","executionInfo":{"status":"ok","timestamp":1565993822005,"user_tz":180,"elapsed":160310,"user":{"displayName":"Victor Souza","photoUrl":"https://lh6.googleusercontent.com/-NwhzXpM6Fpc/AAAAAAAAAAI/AAAAAAAAABU/YTXCcMi3bXQ/s64/photo.jpg","userId":"17774617438617334692"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = './gdrive/My Drive/Colab Notebooks/bd'"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J_3KGxnV831l","colab_type":"text"},"source":["**Modelo: ResNet**"]},{"cell_type":"code","metadata":{"id":"pNTT7Qhk6ymF","colab_type":"code","outputId":"2941b19d-4ca3-4813-c718-b6d392a30736","executionInfo":{"status":"ok","timestamp":1565747455942,"user_tz":180,"elapsed":1601728,"user":{"displayName":"Victor Souza","photoUrl":"https://lh6.googleusercontent.com/-NwhzXpM6Fpc/AAAAAAAAAAI/AAAAAAAAABU/YTXCcMi3bXQ/s64/photo.jpg","userId":"17774617438617334692"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["from __future__ import print_function\n","import keras\n","from keras.layers import Dense, Conv2D, Dropout, Activation\n","from keras.layers import AveragePooling2D, Input, Flatten, MaxPooling2D,GlobalAveragePooling2D\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.regularizers import l2\n","from keras import backend as K\n","from keras.callbacks import LearningRateScheduler\n","from keras.optimizers import SGD\n","from keras.models import Model, Sequential\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n","from keras.metrics import top_k_categorical_accuracy\n","import numpy as np\n","from scipy import ndimage\n","import h5py\n","import matplotlib.pyplot as plt\n","from keras.applications.resnet50 import ResNet50 \n","\n","import os\n","import glob\n","\n","import keras\n","import h5py\n","import os\n","import numpy as np\n","from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n","\n","\n","ext_img = '.jpg'\n","num_px = 75\n","num_py = 75\n","total_imgs = 25000     # total of images considering all classes\n","base_name = '/content/gdrive/My Drive/Colab Notebooks/scene_recognition/bd' # main folder\n","directory_results = '/content/gdrive/My Drive/Colab Notebooks/scene_recognition/Resultados/'\n","model_name= 'resnetCI'\n","\n","\n","num_classes = 10\n","batch_size = 128\n","epochs = 200\n","\n","# Subtracting pixel mean improves accuracy\n","subtract_pixel_mean = True\n","\n","# Load data\n","hf = h5py.File(os.path.join(base_name, 'places%dpx_%dclass_2.h5' % (num_py, num_classes)), 'r')\n","x_train = np.array(hf.get('x_train'))\n","x_test = np.array(hf.get('x_test'))\n","y_train = np.array(hf.get('y_train'))\n","y_test = np.array(hf.get('y_test'))\n","\n","\n","labels_dic = {}\n","for l in hf.keys():\n","    if 'x_train' not in l and 'x_test' not in l and 'y_train' not in l and 'y_test' not in l:\n","        print(hf[l].value)\n","        print(l)\n","        labels_dic[hf[l].value] = l\n","\n","hf.close()\n","del hf\n","print(\"Train shape\" + str(x_train.shape))\n","print(\"Test shape\" + str(x_test.shape))\n","print(\"Label test: \" + str(labels_dic))\n","\n","\n","# Input image dimensions.\n","input_shape = x_train.shape[1:]\n","\n","# Normalize data.\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# If subtract pixel mean is enabled\n","#if subtract_pixel_mean:\n","#    x_train_mean = np.mean(x_train, axis=0)\n","#    x_train -= x_train_mean\n","#    x_test -= x_train_mean\n","\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","print('x_train shape:', x_train.shape)\n","print('y_train shape:', y_train.shape)\n","print('x_test shape:', x_test.shape)\n","print('y_test shape:', y_test.shape)\n","\n","\n","# Convert class vectors to binary class matrices.\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","\n","def lr_schedule(epoch):\n","    \"\"\"Learning Rate Schedule\n","\n","    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n","    Called automatically every epoch as part of callbacks during training.\n","\n","    # Arguments\n","        epoch (int): The number of epochs\n","\n","    # Returns\n","        lr (float32): learning rate\n","    \"\"\"\n","    lr = 1e-3\n","    if epoch > 180:\n","        lr *= 0.5e-3\n","    elif epoch > 160:\n","        lr *= 1e-3\n","    elif epoch > 120:\n","        lr *= 1e-2\n","    elif epoch > 80:\n","        lr *= 1e-1\n","    print('Learning rate: ', lr)\n","    return lr\n","\n","def top_3_accuracy(y_true, y_pred):\n","    return top_k_categorical_accuracy(y_true, y_pred, 3) \n","def top_2_accuracy(y_true, y_pred):\n","    return top_k_categorical_accuracy(y_true, y_pred, 2) \n","\n","base_model = ResNet50(include_top=False, weights='imagenet',input_shape=input_shape,classes=num_classes)#weights=imagenet\n","\n","model = base_model.output\n","model = GlobalAveragePooling2D()(model)\n","model = Dense(200, activation='relu')(model)\n","model = Dropout(0.5)(model)\n","predictions = Dense(num_classes, activation='softmax',kernel_initializer='he_normal')(model)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=SGD(lr=lr_schedule(0), momentum=0.9),\n","              metrics=['accuracy',top_3_accuracy,top_2_accuracy])\n","\n","print(model.summary())\n","# this is the augmentation configuration we will use for training\n","train_datagen = ImageDataGenerator(\n","        # set input mean to 0 over the dataset\n","        featurewise_center=False,\n","        # set each sample mean to 0\n","        samplewise_center=False,\n","        # divide inputs by std of dataset\n","        featurewise_std_normalization=False,\n","        # divide each input by its std\n","        samplewise_std_normalization=False,\n","        # apply ZCA whitening\n","        zca_whitening=False,\n","        # epsilon for ZCA whitening\n","        zca_epsilon=1e-06,\n","        # randomly rotate images in the range (deg 0 to 180)\n","        rotation_range=0,\n","        # randomly shift images horizontally\n","        width_shift_range=0.1,\n","        # randomly shift images vertically\n","        height_shift_range=0.1,\n","        # set range for random shear\n","        shear_range=0.,\n","        # set range for random zoom\n","        zoom_range=0.,\n","        # set range for random channel shifts\n","        channel_shift_range=0.,\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='nearest',\n","        # value used for fill_mode = \"constant\"\n","        cval=0.,\n","        # randomly flip images\n","        horizontal_flip=True,\n","        # randomly flip images\n","        vertical_flip=False,\n","        # set rescaling factor (applied before any other transformation)\n","        rescale=None,\n","        # set function that will be applied on each input\n","        preprocessing_function=None\n",")\n","\n","# this is the augmentation configuration we will use for testing:\n","# only rescaling\n","test_datagen = ImageDataGenerator(rescale=None)\n","\n","# this is a generator that will read pictures found in\n","# subfolers of 'data/train', and indefinitely generate\n","# batches of augmented image data\n","train_generator = train_datagen.flow(x_train, y_train,\n","        batch_size=batch_size)\n","\n","# this is a similar generator, for validation data\n","validation_generator = test_datagen.flow(x_test, y_test,\n","        batch_size=batch_size)\n","lr_scheduler = LearningRateScheduler(lr_schedule)\n","callbacks =  [lr_schedule]\n","h = model.fit_generator(\n","        train_generator,\n","        steps_per_epoch=x_train.shape[0] // batch_size,\n","        epochs=epochs,\n","        validation_data=validation_generator,\n","        validation_steps=x_test.shape[0] // batch_size,\n","        callbacks=[LearningRateScheduler(lr_schedule)]\n",")\n","\n","\n","\n","##Cria uma nova pasta para a respectiva execução\n","\n","if os.path.exists(directory_results+model_name) == False:\n","  os.mkdir(directory_results+model_name)\n","     \n","directory_results = directory_results+model_name\n","tamanho_directory = len(directory_results)+1\n","\n","lista = [os.path.join(directory_results, o) for o in os.listdir(directory_results) \n","                    if os.path.isdir(os.path.join(directory_results,o))]\n","count = 0\n","for pasta in lista:\n","  count=int(pasta[tamanho_directory:])\n","\n","count+=1\n","os.mkdir(directory_results+'/'+str(count))\n","directory_results = directory_results+'/'+str(count)+'/'\n","\n","\n","\n","\n","\n","\n","model.save_weights(os.path.join(directory_results,model_name+'.h5'))#save weights\n","\n","print(h)\n","print(h.history)\n","# History of acc(train) and val_acc(validation)\n","hist_val_top_3_accuracy = h.history['val_top_3_accuracy']\n","hist_val_top_2_accuracy = h.history['val_top_2_accuracy']\n","hist_top_3_accuracy = h.history['top_3_accuracy']\n","hist_top_2_accuracy = h.history['top_2_accuracy']\n","hist_lr = h.history['lr']\n","hist_acc_train = h.history['acc']\n","hist_loss_train = h.history['loss']\n","hist_acc_test = h.history['val_acc']\n","hist_loss_test = h.history['val_loss']\n","\n","print('History of train\\n', hist_acc_train)\n","print('History of test\\n', hist_acc_test)\n","\n","np.save(directory_results+model_name+'_val_top_3_accuracy.npy',hist_val_top_3_accuracy)\n","np.save(directory_results+model_name+'_val_top_2_accuracy.npy',hist_val_top_2_accuracy)\n","np.save(directory_results+model_name+'_top_3_accuracy.npy',hist_top_3_accuracy)\n","np.save(directory_results+model_name+'_top_2_accuracy.npy',hist_top_2_accuracy)\n","np.save(directory_results+model_name+'_lr.npy',hist_lr)\n","np.save(directory_results+model_name+'_acc_train.npy',hist_acc_train)\n","np.save(directory_results+model_name+'_loss_train.npy',hist_loss_train)\n","np.save(directory_results+model_name+'_acc_test.npy',hist_acc_test)\n","np.save(directory_results+model_name+'_loss_test.npy',hist_loss_test)\n","\n","\n","\n","# plot history\n","plt.figure(1)\n","plt.plot(hist_loss_train)\n","plt.plot(hist_loss_test)\n","plt.legend(['Train loss','Test loss'], loc='upper right')\n","plt.title('Learning curves')\n","\n","plt.figure(2)\n","plt.plot(hist_acc_train)\n","plt.plot(hist_acc_test)\n","plt.legend(['Train acc','Test acc'], loc='upper right')\n","plt.title('Accuracy')\n","\n","# Report\n","pred_test = np.argmax(model.predict(x_test, batch_size=1, verbose=0), axis=1)\n","true_test = np.argmax(y_test, axis=1)\n","cf = confusion_matrix(true_test, pred_test)\n","print(cf)\n","\n","np.save(directory_results+model_name+'_pred_test.npy',pred_test)\n","np.save(directory_results+model_name+'_true_test.npy',true_test)\n","np.save(directory_results+model_name+'_cf.npy',cf)\n","\n","\n","if(not os.path.exists(directory_results+model_name+'.txt')):\n","  file = open(directory_results+model_name+'.txt', 'w')\n","  file.close()\n","\n","file = open(directory_results+model_name+'.txt', 'r')\n","\n","lines = file.readlines()\n","\n","results_class = []\n","for cl in range(num_classes):\n","  print(\"\\n%s: %.2f%%\" % (labels_dic[cl], cf[cl,cl]/sum(true_test==cl)*100))\n","  lines.append(\"\\n%s: %.2f%%\" % (labels_dic[cl], cf[cl,cl]/sum(true_test==cl)*100))\n","  results_class.append(cf[cl,cl]/sum(true_test==cl)*100)\n","\n","np.save(directory_results+model_name+'_results_class.npy',results_class)\n","labels = []\n","for i in labels_dic:\n","  labels.append(labels_dic[i])\n","np.save(directory_results+model_name+'_labels.npy',labels)\n","\n","print(labels_dic.values())\n","print(classification_report(true_test, pred_test, target_names=labels_dic.values()))\n","lines.append(\"\\n\"+str(classification_report(true_test, pred_test, target_names=labels_dic.values())))\n","\n","# save file execution\n","file = open(directory_results+model_name+'.txt', 'w')\n","file.writelines(lines)\n","file.close()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]}]}